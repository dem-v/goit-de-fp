"""
Kafka Streaming Consumer –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: Kafka -> Spark Streaming -> [Kafka + MySQL]
–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –∑ —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º –∫–µ—Ä—É–≤–∞–Ω–Ω—è–º –¥–∞–Ω–∏–º–∏
"""

import logging
import signal
import sys
import time
from typing import Optional, Dict, Any
from contextlib import contextmanager
from dataclasses import dataclass

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark.sql.streaming import StreamingQuery

from configs import config

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–æ–±–∫–∏ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É"""
    batches_processed: int = 0
    total_records_processed: int = 0
    total_records_written_kafka: int = 0
    total_records_written_mysql: int = 0
    failed_batches: int = 0

    def log_summary(self):
        logger.info(f"Processing Summary - Batches: {self.batches_processed}, "
                   f"Records processed: {self.total_records_processed}, "
                   f"Kafka writes: {self.total_records_written_kafka}, "
                   f"MySQL writes: {self.total_records_written_mysql}, "
                   f"Failed batches: {self.failed_batches}")


class DataManager:
    """–¶–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞–Ω–∏–º–∏"""

    def __init__(self, spark: SparkSession):
        self.spark = spark
        self._bio_df: Optional[DataFrame] = None
        self._events_df: Optional[DataFrame] = None

    def get_bio_df(self, force_reload: bool = False) -> DataFrame:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è bio DataFrame –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        if self._bio_df is None or force_reload:
            logger.info("Loading bio data from database...")
            self._bio_df = self._load_bio_data()
            self._bio_df.cache()

            record_count = self._bio_df.count()
            logger.info(f"Bio data loaded and cached: {record_count} records")

        return self._bio_df

    def get_events_df(self, force_reload: bool = False) -> DataFrame:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è events DataFrame –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        if self._events_df is None or force_reload:
            logger.info("Loading events data from database...")
            self._events_df = self._load_events_data()
            self._events_df.cache()

            record_count = self._events_df.count()
            logger.info(f"Events data loaded and cached: {record_count} records")

        return self._events_df

    def _load_bio_data(self) -> DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è bio –¥–∞–Ω–∏—Ö –∑ –±–∞–∑–∏"""
        df = (self.spark.read
              .format('jdbc')
              .options(
                  url=config["jdbc.url"],
                  driver=config["jdbc.driver"],
                  dbtable=config["jdbc.bio_table"],
                  user=config["jdbc.user"],
                  password=config["jdbc.password"],
                  numPartitions="2",  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 4 –¥–æ 2
                  fetchsize="500"    # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 1000 –¥–æ 500
              )
              .load())

        return (df.select(
                    col("athlete_id"),
                    col("name"),
                    col("sex"),
                    col("height").cast("float").alias("height"),
                    col("weight").cast("float").alias("weight"),
                    col("country_noc")
                )
                .filter(col("height").isNotNull() & col("weight").isNotNull())
                .filter(col("athlete_id").isNotNull()))  # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è

    def _load_events_data(self) -> DataFrame:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è events –¥–∞–Ω–∏—Ö –∑ –±–∞–∑–∏"""
        df = (self.spark.read
              .format('jdbc')
              .options(
                  url=config["jdbc.url"],
                  driver=config["jdbc.driver"],
                  dbtable=config["jdbc.events_table"],
                  user=config["jdbc.user"],
                  password=config["jdbc.password"],
                  numPartitions="2",  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 4 –¥–æ 2
                  fetchsize="500"    # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 1000 –¥–æ 500
              )
              .load())

        return df.select(
            col("country_noc"),
            col("sport"),
            col("athlete_id"),
            col("medal"),
            col("result_id")
        ).filter(col("athlete_id").isNotNull())

    def cleanup(self):
        """–û—á–∏—â–µ–Ω–Ω—è –∫–µ—à–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        if self._bio_df:
            self._bio_df.unpersist()
        if self._events_df:
            self._events_df.unpersist()
        logger.info("Data cache cleared")


class SparkSessionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è Spark Session –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏"""

    @staticmethod
    def create_session(app_name: str = "AthleteResultsProcessor") -> SparkSession:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó Spark Session"""
        logger.info("Creating optimized Spark Session...")

        spark = (SparkSession.builder
                .appName(app_name)
                .master("local[2]")  # –û–±–º–µ–∂—É—î–º–æ –¥–æ 2 —è–¥–µ—Ä –∑–∞–º—ñ—Å—Ç—å –≤—Å—ñ—Ö
                .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.33,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5")
                .config("spark.driver.memory", "2g")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 4g –¥–æ 1g
                .config("spark.executor.memory", "2g")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 4g –¥–æ 1g
                .config("spark.driver.maxResultSize", "512m")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 2g –¥–æ 512m
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 128MB
                .config("spark.sql.streaming.checkpointLocation.deletedFileRetention", "50")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 100
                .config("spark.sql.streaming.minBatchesToRetain", "5")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 10
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .config("spark.sql.execution.arrow.pyspark.enabled", "false")  # –í–∏–º–∫–Ω–µ–Ω–æ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
                # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
                .config("spark.sql.streaming.metricsEnabled", "false")  # –í–∏–º–∫–Ω—É—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
                .config("spark.sql.ui.retainedExecutions", "5")  # –ó–º–µ–Ω—à–∏—Ç–∏ retained executions
                .config("spark.ui.retainedJobs", "10")  # –ó–º–µ–Ω—à–∏—Ç–∏ retained jobs
                .config("spark.ui.retainedStages", "10")  # –ó–º–µ–Ω—à–∏—Ç–∏ retained stages
                .config("spark.worker.cleanup.enabled", "true")  # –£–≤—ñ–º–∫–Ω—É—Ç–∏ cleanup
                .config("spark.sql.adaptive.localShuffleReader.enabled", "true")  # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è shuffle
                .getOrCreate())

        spark.sparkContext.setLogLevel("WARN")
        logger.info(f"Spark Session created successfully - Version: {spark.version}")

        return spark


class KafkaConnector:
    """–ö–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ Kafka"""

    def __init__(self):
        self.kafka_options = self._build_kafka_options()

    def _build_kafka_options(self) -> Dict[str, str]:
        """–ü–æ–±—É–¥–æ–≤–∞ –æ–ø—Ü—ñ–π Kafka –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è"""
        return {
            "kafka.bootstrap.servers": config['kafka.bootstrap_servers'][0],
            "kafka.security.protocol": config['kafka.security_protocol'],
            "kafka.sasl.mechanism": config['kafka.sasl_mechanism'],
            "kafka.sasl.jaas.config": (
                f"org.apache.kafka.common.security.plain.PlainLoginModule required "
                f"username='{config['kafka.username']}' "
                f"password='{config['kafka.password']}';"
            ),
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
            "kafka.session.timeout.ms": "30000",
            "kafka.request.timeout.ms": "40000",
            "kafka.retry.backoff.ms": "1000"
        }

    def create_input_stream(self, spark: SparkSession) -> DataFrame:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—Ö—ñ–¥–Ω–æ–≥–æ Kafka —Å—Ç—Ä—ñ–º—É"""
        logger.info(f"Creating Kafka input stream for topic: {config['kafka.in_topic']}")

        return (spark.readStream
                .format("kafka")
                .options(**self.kafka_options)
                .option("subscribe", config['kafka.in_topic'])
                .option("startingOffsets", "earliest")
                .option("maxOffsetsPerTrigger", "20")  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 100 –¥–æ 20
                .option("failOnDataLoss", "false")
                .option("includeHeaders", "false")  # –ù–µ –≤–∫–ª—é—á–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —è–∫—â–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ
                .load())


def retry_on_failure(max_attempts: int = 3, delay: float = 1):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è retry –ª–æ–≥—ñ–∫–∏"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        logger.warning(f"üîÑ Retry {attempt + 1}/{max_attempts} for {func.__name__}: {str(e)}")
                        time.sleep(delay * (attempt + 1))  # Exponential backoff
                    else:
                        logger.error(f"üí• All {max_attempts} attempts failed for {func.__name__}")
            raise last_exception
        return wrapper
    return decorator


class DataTransformer:
    """–ö–ª–∞—Å –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö"""

    @staticmethod
    def get_json_schema() -> StructType:
        """JSON —Å—Ö–µ–º–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""
        return StructType([
            StructField("athlete_id", IntegerType(), True),
            StructField("sport", StringType(), True),
            StructField("medal", StringType(), True),
            StructField("country_noc", StringType(), True),
            StructField("result_id", IntegerType(), True)
        ])

    @classmethod
    def parse_kafka_messages(cls, df: DataFrame) -> DataFrame:
        """–ü–∞—Ä—Å–∏–Ω–≥ Kafka –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""
        json_schema = cls.get_json_schema()

        return (df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
                .withColumn("value_json", from_json(col("value"), json_schema))
                .select(
                    coalesce(
                        col("value_json.result_id").cast("bigint"),
                        col("key").cast("bigint")
                    ).alias("result_id"),
                    col("value_json.athlete_id").alias("athlete_id"),
                    col("value_json.sport").alias("sport"),
                    col("value_json.medal").alias("medal"),
                    col("value_json.country_noc").alias("country_noc")
                )
                .filter(col("athlete_id").isNotNull() & col("sport").isNotNull()))

    @staticmethod
    def aggregate_athlete_stats(df: DataFrame, bio_df: DataFrame) -> DataFrame:
        """–ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ñ–≤"""
        return (df.join(bio_df, "athlete_id", "inner")
                .drop(bio_df.country_noc)  # –í–∏–¥–∞–ª—è—î–º–æ country_noc –∑ bio_df, –∑–∞–ª–∏—à–∞—î–º–æ –∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ df
                .select('sport', 'medal', 'sex', 'country_noc', 'height', 'weight')
                .withColumn("processing_timestamp", current_timestamp())
                .groupBy(['sport', 'medal', 'sex', 'country_noc'])
                .agg(
                    count("*").alias("total_athletes"),
                    avg("height").alias("avg_height"),
                    avg("weight").alias("avg_weight"),
                    min("height").alias("min_height"),
                    max("height").alias("max_height"),
                    min("weight").alias("min_weight"),
                    max("weight").alias("max_weight"),
                    first("processing_timestamp").alias("timestamp")
                ))

    @staticmethod
    def prepare_kafka_output(df: DataFrame) -> DataFrame:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∏–≤–æ–¥—É –≤ Kafka"""
        return (df.withColumn(
                    "key",
                    concat(
                        col("sport"), lit("|"),
                        col("medal"), lit("|"),
                        col("sex"), lit("|"),
                        col("country_noc")
                    )
                )
                .select(
                    col("key").cast("string"),
                    to_json(struct(
                        "sport", "medal", "sex", "country_noc",
                        "total_athletes", "avg_height", "avg_weight",
                        "min_height", "max_height", "min_weight", "max_weight",
                        "timestamp"
                    )).alias("value")
                ))


class BatchProcessor:
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å–æ—Ä –±–∞—Ç—á—ñ–≤"""

    def __init__(self, data_manager: DataManager, kafka_connector: KafkaConnector, metrics: ProcessingMetrics):
        self.data_manager = data_manager
        self.kafka_connector = kafka_connector
        self.metrics = metrics

    def process_batch(self, batch_df: DataFrame, batch_id: int) -> None:
        """–û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –æ–±—Ä–æ–±–∫–∏ –±–∞—Ç—á—É"""
        batch_start_time = time.time()

        try:
            logger.info(f"üîÑ Processing batch {batch_id}")

            if batch_df.rdd.isEmpty():
                logger.info(f"üì≠ Batch {batch_id} is empty, skipping")
                return

            input_count = batch_df.count()
            logger.info(f"üì• Batch {batch_id}: {input_count} input records")

            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–≤—ñ–¥–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            bio_df = self.data_manager.get_bio_df()

            # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
            aggregated_df = DataTransformer.aggregate_athlete_stats(batch_df, bio_df)

            if aggregated_df.rdd.isEmpty():
                logger.warning(f"‚ö†Ô∏è Batch {batch_id}: No records after aggregation")
                return

            output_count = aggregated_df.count()
            logger.info(f"üìä Batch {batch_id}: {output_count} aggregated records")

            # –ü–æ–∫–∞–∑–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –¥–µ–±–∞–≥—É
            if logger.isEnabledFor(logging.DEBUG):
                aggregated_df.show(10, truncate=False)

            # –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            kafka_success, mysql_success = self._write_results(aggregated_df, batch_id)

            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
            self.metrics.batches_processed += 1
            self.metrics.total_records_processed += input_count

            if kafka_success:
                self.metrics.total_records_written_kafka += output_count
            if mysql_success:
                self.metrics.total_records_written_mysql += output_count

            batch_time = time.time() - batch_start_time
            logger.info(f"‚úÖ Batch {batch_id} completed in {batch_time:.2f}s")

        except Exception as e:
            self.metrics.failed_batches += 1
            logger.error(f"‚ùå Error processing batch {batch_id}: {str(e)}", exc_info=True)

    def _write_results(self, df: DataFrame, batch_id: int) -> tuple[bool, bool]:
        """–ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É Kafka —Ç–∞ MySQL"""
        kafka_success = self._write_to_kafka(df, batch_id)
        mysql_success = self._write_to_mysql(df, batch_id)
        return kafka_success, mysql_success

    @retry_on_failure(max_attempts=3, delay=2)
    def _write_to_kafka(self, df: DataFrame, batch_id: int) -> bool:
        """–ó–∞–ø–∏—Å —É Kafka"""
        try:
            kafka_df = DataTransformer.prepare_kafka_output(df)

            (kafka_df.write
             .format("kafka")
             .options(**self.kafka_connector.kafka_options)
             .option("topic", config['kafka.out_topic'])
             .save())

            logger.info(f"üì§ Kafka write successful for batch {batch_id}")
            return True

        except Exception as e:
            logger.error(f"üì§‚ùå Kafka write failed for batch {batch_id}: {str(e)}")
            raise

    @retry_on_failure(max_attempts=3, delay=2)
    def _write_to_mysql(self, df: DataFrame, batch_id: int) -> bool:
        """–ó–∞–ø–∏—Å —É MySQL"""
        try:
            (df.write
             .format("jdbc")
             .options(
                url=config["jdbc.url"],
                driver=config["jdbc.driver"],
                dbtable=config["jdbc.output_table"],
                user=config["jdbc.user"],
                password=config["jdbc.password"],
                batchsize="500",  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 1000 –¥–æ 500
                isolationLevel="READ_UNCOMMITTED"  # –î–ª—è –∫—Ä–∞—â–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
             )
             .mode("append")
             .save())

            logger.info(f"üóÑÔ∏è MySQL write successful for batch {batch_id}")
            return True

        except Exception as e:
            logger.error(f"üóÑÔ∏è‚ùå MySQL write failed for batch {batch_id}: {str(e)}")
            raise


class StreamingApplication:
    """–ì–æ–ª–æ–≤–Ω–∏–π –∫–ª–∞—Å –¥–æ–¥–∞—Ç–∫—É"""

    def __init__(self):
        self.spark: Optional[SparkSession] = None
        self.data_manager: Optional[DataManager] = None
        self.kafka_connector: Optional[KafkaConnector] = None
        self.streaming_query: Optional[StreamingQuery] = None
        self.metrics = ProcessingMetrics()
        self.shutdown_requested = False

        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ–±—Ä–æ–±–∫–∏ —Å–∏–≥–Ω–∞–ª—ñ–≤
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """Graceful shutdown handler"""
        logger.info(f"üõë Received signal {signum}, initiating shutdown...")
        self.shutdown_requested = True

    def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤"""
        logger.info("üöÄ Initializing Streaming Application...")

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Spark Session
        self.spark = SparkSessionManager.create_session()

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ–Ω–µ–¥–∂–µ—Ä—ñ–≤
        self.data_manager = DataManager(self.spark)
        self.kafka_connector = KafkaConnector()

        # –ü—Ä–æ–≥—Ä—ñ–≤–∞–Ω–Ω—è –∫–µ—à—É –¥–∞–Ω–∏—Ö
        self.data_manager.get_bio_df()

        logger.info("‚úÖ Application initialized successfully")

    def start_streaming(self):
        """–ó–∞–ø—É—Å–∫ streaming –æ–±—Ä–æ–±–∫–∏"""
        logger.info(f"‚ñ∂Ô∏è Starting streaming for topic: {config['kafka.in_topic']}")

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—Ä—ñ–º—É
        input_stream = self.kafka_connector.create_input_stream(self.spark)
        transformed_stream = DataTransformer.parse_kafka_messages(input_stream)

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è batch processor
        batch_processor = BatchProcessor(self.data_manager, self.kafka_connector, self.metrics)

        # –ó–∞–ø—É—Å–∫ streaming query
        self.streaming_query = (
            transformed_stream.writeStream
            .foreachBatch(batch_processor.process_batch)
            .outputMode("update")
            .option("checkpointLocation", "/tmp/athlete-streaming-checkpoints")
            .trigger(processingTime="30 seconds")  # –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 20 –¥–æ 30 —Å–µ–∫—É–Ω–¥
            .start()
        )

        logger.info(f"üéØ Streaming query started - ID: {self.streaming_query.id}")

        # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
        self._monitor_streaming()

    def _monitor_streaming(self):
        """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ streaming job"""
        last_metrics_log = time.time()

        try:
            while not self.shutdown_requested and self.streaming_query.isActive:
                time.sleep(5)

                # –õ–æ–≥—É—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–∂–Ω—ñ 60 —Å–µ–∫—É–Ω–¥
                if time.time() - last_metrics_log > 60:
                    self.metrics.log_summary()
                    last_metrics_log = time.time()

        except KeyboardInterrupt:
            logger.info("‚å®Ô∏è Keyboard interrupt received")
        finally:
            self._shutdown()

    def _shutdown(self):
        """Graceful shutdown"""
        logger.info("üõë Shutting down application...")

        if self.streaming_query and self.streaming_query.isActive:
            logger.info("‚èπÔ∏è Stopping streaming query...")
            self.streaming_query.stop()

        if self.data_manager:
            self.data_manager.cleanup()

        if self.spark:
            logger.info("üîå Stopping Spark session...")
            self.spark.stop()

        # –§—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        self.metrics.log_summary()
        logger.info("‚úÖ Application shutdown completed")

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫—É"""
        try:
            self.initialize()
            self.start_streaming()
        except Exception as e:
            logger.error(f"üí• Application error: {str(e)}", exc_info=True)
            raise
        finally:
            self._shutdown()


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥—É"""
    app = StreamingApplication()

    try:
        app.run()
    except KeyboardInterrupt:
        logger.info("üëã Application stopped by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"üíÄ Fatal error: {str(e)}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()